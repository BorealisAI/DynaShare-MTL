# Copyright (c) 2022-present, Royal Bank of Canada.
# Copyright (c) 2022-present, Ximeng Sun
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
##################################################################################################
# Code is based on the AdaShare (https://arxiv.org/pdf/1911.12423.pdf) implementation 
# from https://github.com/sunxm2357/AdaShare by Ximeng Sun
##################################################################################################


import os
import pandas as pd
from torch import Tensor
from torch.utils.data import  DataLoader, TensorDataset

PATH = "/home/MTL/data/pcba/"

try:
    """
    Deeochem is required only if working with PCBA
    reference: https://github.com/deepchem/deepchem
    """
    import gzip
    import deepchem
except ImportError:
    print("Deepchem not installed")


def create_pcba_dataset(
    featurizer="ECFP",
    split="random",
    reload=True,
    assay_file_name="pcba.csv.gz",
    data_dir=None,
    save_dir=None,
    **kwargs
):
    """
    DATASET 3: PCBA
    For Download and pre-processing, follow:
    #https://github.com/chao1224/Loss-Balanced-Task-Weighting/blob/master/src/pcba_model.py
    #https://github.com/deepchem/deepchem/blob/6c5a5405acea333ee7a65a798ddb5c9df702a0b8/deepchem/molnet/load_function/pcba_datasets.py#L62

    After downloading, before fitting the model, run:
    python data_pcba.py to create the features
    Note: Needs a different enviroment, check the data_pcba.py file for more instructions
    """

    """
    Load PCBA dataset
    PubChem BioAssay (PCBA) is a database consisting of biological activities of
    small molecules generated by high-throughput screening. We use a subset of
    PCBA, containing 128 bioassays measured over 400 thousand compounds,
    used by previous work to benchmark machine learning methods.
    Random splitting is recommended for this dataset.
    The raw data csv file contains columns below:
    - "mol_id" - PubChem CID of the compound
    - "smiles" - SMILES representation of the molecular structure
    - "PCBA-XXX" - Measured results (Active/Inactive) for bioassays:
        search for the assay ID at
        https://pubchem.ncbi.nlm.nih.gov/search/#collection=bioassays
        for details
    References
    ----------
    .. [1] Wang, Yanli, et al. "PubChem's BioAssay database."
     Nucleic acids research 40.D1 (2011): D400-D412.
    """

    if data_dir is None:
        data_dir = DEFAULT_DIR
    if save_dir is None:
        save_dir = DEFAULT_DIR

    if reload:
        save_folder = os.path.join(
            save_dir, assay_file_name.split(".")[0] + "-featurized", featurizer
        )
        if featurizer == "smiles2img":
            img_spec = kwargs.get("img_spec", "std")
            save_folder = os.path.join(save_folder, img_spec)
        save_folder = os.path.join(save_folder, str(split))

    dataset_file = os.path.join(data_dir, assay_file_name)

    if not os.path.exists(dataset_file):
        print("File does not exist!")

    # Featurize PCBA dataset
    if featurizer == "ECFP":
        featurizer = deepchem.feat.CircularFingerprint(size=1024)
    elif featurizer == "GraphConv":
        featurizer = deepchem.feat.ConvMolFeaturizer()
    elif featurizer == "Weave":
        featurizer = deepchem.feat.WeaveFeaturizer()
    elif featurizer == "Raw":
        featurizer = deepchem.feat.RawFeaturizer()
    elif featurizer == "smiles2img":
        img_spec = kwargs.get("img_spec", "std")
        img_size = kwargs.get("img_size", 80)
        featurizer = deepchem.feat.SmilesToImage(img_size=img_size, img_spec=img_spec)

    with gzip.GzipFile(dataset_file, "r") as fin:
        header = fin.readline().rstrip().decode("utf-8")
        columns = header.split(",")
        columns.remove("mol_id")
        columns.remove("smiles")
        PCBA_tasks = columns

    if reload:
        (
            loaded,
            all_dataset,
            transformers,
        ) = deepchem.utils.data_utils.load_dataset_from_disk(save_folder)
        if loaded:
            return PCBA_tasks, all_dataset, transformers

    loader = deepchem.data.data_loader.CSVLoader(
        tasks=PCBA_tasks, smiles_field="smiles", featurizer=featurizer
    )

    dataset = loader.featurize(dataset_file)

    if split == None:
        transformers = [deepchem.trans.BalancingTransformer(dataset=dataset)]
        print("Split is None, about to transform data")
        for transformer in transformers:
            dataset = transformer.transform(transform_X=True, dataset=dataset)

        return PCBA_tasks, (dataset, None, None), transformers

    splitters = {
        "index": deepchem.splits.IndexSplitter(),
        "random": deepchem.splits.RandomSplitter(),
        "scaffold": deepchem.splits.ScaffoldSplitter(),
        "stratified": deepchem.splits.SingletaskStratifiedSplitter(),
    }
    splitter = splitters[split]
    frac_train = kwargs.get("frac_train", 0.8)
    frac_valid = kwargs.get("frac_valid", 0.1)
    frac_test = kwargs.get("frac_test", 0.1)

    train, valid, test = splitter.train_valid_test_split(
        dataset, frac_train=frac_train, frac_valid=frac_valid, frac_test=frac_test
    )

    transformers = [
        deepchem.trans.BalancingTransformer(transform_w=True, dataset=train)
    ]

    for transformer in transformers:
        train = transformer.transform(train)
        valid = transformer.transform(valid)
        test = transformer.transform(test)

    if reload:
        deepchem.utils.data_utils.save_dataset_to_disk(
            save_folder, train, valid, test, transformers
        )

    return PCBA_tasks, (train, valid, test), transformers


def load_pcba(
    featurizer="ECFP",
    split="random",
    reload=True,
    data_dir=None,
    save_dir=None,
    **kwargs
):
    return create_pcba_dataset(
        featurizer=featurizer,
        split=split,
        reload=reload,
        assay_file_name="pcba.csv.gz",
        data_dir=data_dir,
        save_dir=save_dir,
        **kwargs
    )


def run_dataprep(path=PATH):
    """
    Run this function in a separate virtualenv due to compatibility issues
    """

    DEFAULT_DIR = path  # /home/MTL/data/pcba

    data = pd.read_csv(DEFAULT_DIR + "/pcba.csv.gz")
    print("Raw data shape:")
    print(data.shape)

    tasks, data, transform = load_pcba(
        featurizer="ECFP",
        split="random",
        reload=False,
        data_dir=DEFAULT_DIR,
        save_dir=DEFAULT_DIR,
        kwards={"frac_train": 0.7, "frac_valid": 0.15, "frac_test": 0.15},
    )

    train, val, test = data
    print(train.X.shape, train.y.shape, train.w.shape)
    print(val.X.shape, val.y.shape, val.w.shape)
    print(test.X.shape, test.y.shape, test.w.shape)

    print("... Saving files in csv and csv.gz files")
    pd.DataFrame(train.X).to_csv(DEFAULT_DIR + "pcba_train_features.csv", index=False)
    pd.DataFrame(test.X).to_csv(DEFAULT_DIR + "pcba_test_features.csv", index=False)
    pd.DataFrame(val.X).to_csv(DEFAULT_DIR + "pcba_val_features.csv", index=False)

    pd.DataFrame(train.y).to_csv(DEFAULT_DIR + "pcba_train_y.csv", index=False)
    pd.DataFrame(test.y).to_csv(DEFAULT_DIR + "pcba_test_y.csv", index=False)
    pd.DataFrame(val.y).to_csv(DEFAULT_DIR + "pcba_val_y.csv", index=False)

    pd.DataFrame(train.X).to_csv(
        DEFAULT_DIR + "pcba_train_features.csv.gz", index=False
    )
    pd.DataFrame(test.X).to_csv(DEFAULT_DIR + "pcba_test_features.csv.gz", index=False)
    pd.DataFrame(val.X).to_csv(DEFAULT_DIR + "pcba_val_features.csv.gz", index=False)

    pd.DataFrame(train.y).to_csv(DEFAULT_DIR + "pcba_train_y.csv.gz", index=False)
    pd.DataFrame(test.y).to_csv(DEFAULT_DIR + "pcba_test_y.csv.gz", index=False)
    pd.DataFrame(val.y).to_csv(DEFAULT_DIR + "pcba_val_y.csv.gz", index=False)

    pd.DataFrame(train.w).to_csv(DEFAULT_DIR + "pcba_train_w.csv.gz", index=False)
    pd.DataFrame(test.w).to_csv(DEFAULT_DIR + "pcba_test_w.csv.gz", index=False)
    pd.DataFrame(val.w).to_csv(DEFAULT_DIR + "pcba_val_w.csv.gz", index=False)

    pd.DataFrame(tasks).to_csv(DEFAULT_DIR + "tasks.csv", index=False)


def PCBADataset(X, y, task_list, name, batch, prop, w=None):
    X.fillna(0, inplace=True)
    y.fillna(0, inplace=True)
    w.fillna(0, inplace=True)

    print("...", name, " X shape", X.shape, "and y shape", y.shape)

    if prop < 1:
        size = int(X.shape[0] * prop)
        X = X.values[0:size, :]
        y = y.values[0:size, :]
        w = w.values[0:size, :]
        print("Using prop", " X shape", X.shape, "and y shape", y.shape)
    else:
        X = X.values
        y = y.values
        w = w.values

    dataset = TensorDataset(Tensor(X), Tensor(y), Tensor(w))

    """ Required: Create DataLoader for training the models """
    return DataLoader(dataset, shuffle=True, batch_size=batch)


def data_preparation_pcba(params, path=PATH):
    data_directory = params["dataload"]["dataroot"]

    train_X = pd.read_csv(data_directory + "pcba_train_features.csv.gz")
    val_X = pd.read_csv(data_directory + "pcba_val_features.csv.gz")
    test_X = pd.read_csv(data_directory + "pcba_test_features.csv.gz")

    train_y = pd.read_csv(data_directory + "pcba_train_y.csv.gz")
    val_y = pd.read_csv(data_directory + "pcba_val_y.csv.gz")
    test_y = pd.read_csv(data_directory + "pcba_test_y.csv.gz")

    train_w = pd.read_csv(data_directory + "pcba_train_w.csv.gz")
    val_w = pd.read_csv(data_directory + "pcba_val_w.csv.gz")
    test_w = pd.read_csv(data_directory + "pcba_test_w.csv.gz")

    task_list = pd.read_csv(data_directory + "tasks.csv")
    task_list = task_list.values.reshape(1, -1)
    num_features = train_X.shape[1]
    num_tasks = train_y.shape[1]

    """ Required: Create DataLoader for training the models """
    train_loader = PCBADataset(
        X=train_X,
        y=train_y,
        task_list=task_list,
        name="Train",
        batch=params["train"]["batch_size"],
        prop=params["train"]["prop"],
        w=train_w,
    )
    val_loader = PCBADataset(
        X=val_X,
        y=val_y,
        task_list=task_list,
        name="Validation",
        batch=params["train"]["batch_size"],
        prop=params["train"]["prop"],
        w=val_w,
    )
    test_loader = PCBADataset(
        X=test_X,
        y=test_y,
        task_list=task_list,
        name="Test",
        batch=params["train"]["batch_size"],
        prop=params["train"]["prop"],
        w=test_w,
    )
    return train_loader, val_loader, test_loader, num_features, num_tasks, task_list[0]

