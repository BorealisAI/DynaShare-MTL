exp_name: pcba_test_batch512_resnet18_train_60000epochs_lr0001_no_decay_re-train0.8 #pcba_test_batch128_resnet18_train_60000epochs_lr003_no_decay
seed: [88, 45, 50, 100, 44, 48, 2048, 2222, 23, 12, 94, 88, 1000, 42, 77, 666]
backbone: ResNet18 # ResNet34
#tasks: [ multilabel sequence classification (phenotyping), regression (length-of-stay, time-series), classification task (in-hospital Mortality), time-series classification (decompensation)]
tasks: '' #read from dataset
tasks_num_class: '' # set to one for all tasks in the train.py code 
lambdas: '' #task weights, set to one in the base_env.py (get_pcba_loss) code 
target_rates:  [1, 1, 1, 0.8, 0.8, 0.8, 0.8, 1] #[1, 1, 1, 1, 1, 1, 0.9, 1] # [1, 1, 1, 0.7, 0.7, 0.7, 0.7, 1] [1, 1, 1, 0.4, 0.4, 0.4, 0.4, 1], [1, 1, 1, 1, 1, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 1, 1]
policy_model: AIG # options: task-specific(regular 1d), AIG, Dynamic, AIGDynamic

alpha: 0.00001 #decay downgrad
task_balance_method: 'None' #options: 'None', 'DWA', 'LBTW'

paths:
  log_dir: ../experiments/logs/
  result_dir: ../experiments/results
  checkpoint_dir: ../experiments/checkpoints


dataload:
  dataset: pcba
  dataroot: "/home/MTL/data/pcba/"


policy: True
init_neg_logits:
is_sparse: True
is_sharing: True
skip_layer: 0
is_curriculum: True
curriculum_speed: 3
fix_BN: False
diff_sparsity_weights: True
retrain_from_pl: False


train:
  prop: 0.1 # proportion of training set used
  batch_size: 128
  total_iters: 60000
  warm_up_iters: 59000
  policy_learning_checking: 10
  lr: 0.0001 # 0.001
  policy_lr: 0.001 #0.01
  backbone_lr: 0.0001 #0.001
  reg_w: 0.001
  reg_w_hamming: 0.05
  targetrate_w: 25 #target rate loss weights [2,25,100]
  print_freq: 50
  val_freq: 50 
  decay_lr_freq: 50000
  decay_lr_rate: 0.5
  decay_temp_freq: 5
  init_temp: 5
  decay_temp: 0.965
  cw_pcba: 100
  resume: False #True 
  retrain_resume: False
  policy_iter: latest # options: warmup, latest, best
  which_iter: latest #warmup
  init_method: equal
  hard_sampling: False
  

test:
  which_iter: best